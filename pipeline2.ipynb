{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d7f8704",
   "metadata": {},
   "source": [
    "Runs every **prompt** in `prompts/prompts.json` against every **dataset** in `data/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf57e43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# Ensure the project root is on the path\n",
    "PROJECT_ROOT = os.path.dirname(os.path.abspath('__file__'))\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "from pipeline import (\n",
    "    run_prompt_dataset_matrix,\n",
    "    spec_with_inline_data,\n",
    "    load_json_file,\n",
    "    _divider,\n",
    ")\n",
    "import json\n",
    "print('pipeline.py imported successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b36840",
   "metadata": {},
   "source": [
    "Run Batch Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ab35d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_results = run_prompt_dataset_matrix(\n",
    "    data_dir=\"data\",\n",
    "    prompts_file=\"prompts/prompts.json\",\n",
    "    output_file=\"generatedViz/run_results.json\",\n",
    "    specs_dir=\"generatedViz/specs\",\n",
    "    max_retries=5,\n",
    "    ollama_base=\"http://localhost:11434\",\n",
    "    model_name=\"mistral\",\n",
    "    prompt_limit=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0606e38",
   "metadata": {},
   "source": [
    "Inspect the DSPy prompt for each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4052be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in run_results:\n",
    "    _divider(f\"{r['dataset_name']} × {r['prompt_id']}\")\n",
    "    for i, attempt in enumerate(r.get(\"attempts\", []), 1):\n",
    "        print(f\"\\n  ── Attempt {i} ({'OK' if attempt.get('success') else 'FAIL'}) ──\")\n",
    "        for msg in attempt.get(\"prompt_messages\", []):\n",
    "            role = msg.get(\"role\", \"?\").upper()\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            # Truncate long content for readability\n",
    "            if len(content) > 500:\n",
    "                content = content[:500] + \"...[truncated]\"\n",
    "            print(f\"    [{role}] {content}\")\n",
    "        if attempt.get(\"error\"):\n",
    "            print(f\"    ERROR: {attempt['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ff366d",
   "metadata": {},
   "source": [
    "Render ALL valid charts\n",
    "Each chart is labeled with `dataset × prompt_id` and the attempt number it succeeded on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c18ddbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "rendered_count = 0\n",
    "\n",
    "for r in run_results:\n",
    "    if not r[\"is_valid\"] or not r.get(\"spec_dict\"):\n",
    "        continue\n",
    "\n",
    "    dataset_path = r[\"dataset\"]\n",
    "    records = load_json_file(dataset_path)\n",
    "    render_spec = spec_with_inline_data(r[\"spec_dict\"], records)\n",
    "\n",
    "    label = f\"### {r['dataset_name']} × `{r['prompt_id']}` (attempt {r['total_attempts']})\"\n",
    "    display(Markdown(label))\n",
    "\n",
    "    try:\n",
    "        chart = alt.Chart.from_dict(render_spec)\n",
    "        display(chart)\n",
    "        rendered_count += 1\n",
    "    except Exception as e:\n",
    "        print(f\"  Render failed: {e}\")\n",
    "\n",
    "print(f\"\\nRendered {rendered_count} charts out of {len([r for r in run_results if r['is_valid']])} valid specs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f632e4c",
   "metadata": {},
   "source": [
    "Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6193d36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = [r for r in run_results if r[\"is_valid\"]]\n",
    "failed = [r for r in run_results if not r[\"is_valid\"]]\n",
    "\n",
    "print(f\"Total runs : {len(run_results)}\")\n",
    "print(f\"Valid      : {len(valid)}\")\n",
    "print(f\"Failed     : {len(failed)}\")\n",
    "print()\n",
    "\n",
    "# Per-dataset breakdown\n",
    "from collections import Counter\n",
    "ds_valid = Counter(r[\"dataset_name\"] for r in valid)\n",
    "ds_total = Counter(r[\"dataset_name\"] for r in run_results)\n",
    "print(f\"{'Dataset':<20} {'Valid':>6} {'Total':>6} {'Rate':>8}\")\n",
    "print(\"─\" * 42)\n",
    "for ds in sorted(ds_total):\n",
    "    v, t = ds_valid.get(ds, 0), ds_total[ds]\n",
    "    print(f\"{ds:<20} {v:>6} {t:>6} {v/t:>7.0%}\")\n",
    "\n",
    "# Per-prompt breakdown\n",
    "print()\n",
    "pid_valid = Counter(r[\"prompt_id\"] for r in valid)\n",
    "pid_total = Counter(r[\"prompt_id\"] for r in run_results)\n",
    "print(f\"{'Prompt ID':<30} {'Valid':>6} {'Total':>6} {'Rate':>8}\")\n",
    "print(\"─\" * 52)\n",
    "for pid in sorted(pid_total):\n",
    "    v, t = pid_valid.get(pid, 0), pid_total[pid]\n",
    "    print(f\"{pid:<30} {v:>6} {t:>6} {v/t:>7.0%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
